{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基礎文字前處理與關鍵詞搜尋\n",
    "## 本範例重點\n",
    "1. 文字前處理\n",
    "    - 移除標點符號\n",
    "    - 展開縮寫\n",
    "    - 統一轉換為小寫\n",
    "2. 關鍵詞搜尋\n",
    "    - NLTK\n",
    "    - Wordcloud\n",
    "    - 單純貝氏分類器\n",
    "    - KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 匯入相關套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import Counter as cnt\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 讀取檔案\n",
    "\n",
    "本範例使用 [Pang & Lee Movie Reivew (Cornell Movie Review) Dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取主要工作路徑\n",
    "cwd = Path.cwd().parent\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將工作路徑指定到資料集\n",
    "data_path = cwd / 'data' / 'csv' / 'polarity_dataset.csv'\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取CSV檔\n",
    "file_src = pd.read_csv(str(data_path), index_col=0).reset_index(drop=True)\n",
    "file_src.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取CSV檔\n",
    "# 透過 Github 網址讀取 CSV 檔\n",
    "\n",
    "# 1. 未經處理的原始資料\n",
    "#file_src = pd.read_csv('https://raw.githubusercontent.com/eccmyang/Tutorials-for-AIIS/main/data/csv/polarity_dataset.csv?token=GHSAT0AAAAAACF3SY22LM2Z7QZ4BTENRUKEZGMFSSQ', index_col=0).reset_index(drop=True)\n",
    "# 2. 經過處理的資料\n",
    "#file_src = pd.read_csv('https://raw.githubusercontent.com/eccmyang/Tutorials-for-AIIS/main/data/csv/preprocessed_polarity_dataset.csv?token=GHSAT0AAAAAACF3SY22VPLXBQR5OGCE5NDWZGMFS6Q', index_col=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#file_src.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認讀取資料大小\n",
    "file_src.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 縮寫詞表\n",
    "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
    "           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n",
    "           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "           \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前處理函式\n",
    "def clean_text(text, lemmatize = True):\n",
    "    soup = BeautifulSoup(text, \"lxml\") # 移除HTML標籤\n",
    "    text = soup.get_text()\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) # 展開聊天詞與縮寫詞\n",
    "    emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # 表情符號\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # 一般符號與象形符號\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # 交通與地圖符號\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # 國旗\n",
    "                           u\"\\U00002702-\\U000027B0\"  # 各式可能造成空白位元的符號\n",
    "                           u\"\\U000024C2-\\U0001F251\"  # 各式可能造成空白位元的符號\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_clean.sub(r'',text)\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ',text) # 於句點後加空格，以便區分句子段落\n",
    "    text = re.sub(r'http\\S+', '', text) # 移除網址前綴\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) # 標點符號、括號等\n",
    "    #tokens = re.split('\\W+', text) #create tokens\n",
    "    #if lemmatize:\n",
    "    #    text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n",
    "    #else:\n",
    "    #    text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從讀取檔案複製一份來做前處理\n",
    "cpfile = file_src\n",
    "cpfile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 `.apply()` 方法將前處理函式套用到DataFrame中的評論欄位\n",
    "cpfile['review'] = cpfile['review'].apply(clean_text, lemmatize=True)\n",
    "\n",
    "# 移除 `\\n` 換行符號\n",
    "cpfile['review'] = cpfile['review'].replace('\\n', '', regex=True)\n",
    "\n",
    "# 移除 `http` 網址前綴\n",
    "cpfile['review'] = cpfile['review'].replace('http', '', regex=True)\n",
    "\n",
    "# 移除 `www` 網址開頭\n",
    "cpfile['review'] = cpfile['review'].replace('www', '', regex=True)\n",
    "\n",
    "# 移除 `com` 網址結尾\n",
    "cpfile['review'] = cpfile['review'].replace('com', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpfile.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前處理完後，再複製一份前處理後的資料\n",
    "preprocessed = cpfile\n",
    "preprocessed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以將前處理過的資料存成csv檔，之後就不用再跑前處理的程式碼\n",
    "#preprocessed.to_csv(str(cwd / 'data' / 'csv' / 'preprocessed_polarity_dataset.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文字雲 (WordCloud) 與 詞頻 (Frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了尋找關鍵詞，需要先把停用詞去除\n",
    "# 避免在尋找的過程中，一直尋找到停用詞\n",
    "en_stopw = set(stopwords.words(\"english\"))\n",
    "\n",
    "# 讀取去除停用詞後的評論之函式\n",
    "def get_words(review, words, stopw=en_stopw):\n",
    "    tok_rev = wt(review)\n",
    "    rev_word = [word for word in tok_rev if word not in stopw]\n",
    "    words += rev_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正面情感評論\n",
    "pos_rev = preprocessed[preprocessed.sentiment == 1]\n",
    "\n",
    "pos_rev.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一串列儲存正面關鍵詞\n",
    "pos_words = []\n",
    "\n",
    "pos_rev.review.apply(get_words, args=(pos_words,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負面評論\n",
    "neg_rev = preprocessed[preprocessed.sentiment == 0]\n",
    "\n",
    "neg_rev.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一串列儲存負面關鍵詞\n",
    "neg_words = []\n",
    "\n",
    "neg_rev.review.apply(get_words, args=(neg_words,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字雲顯示函式\n",
    "def word_cloud(words):\n",
    "    words_sen = ' '.join(words)\n",
    "    words_wc = WordCloud(width=1920, height=1080).generate(words_sen)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6), facecolor='k')\n",
    "    plt.imshow(words_wc)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正面關鍵詞文字雲\n",
    "pos_words_wordcloud = word_cloud(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 負面關鍵詞文字雲\n",
    "neg_words_wordcloud = word_cloud(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正面詞彙頻率\n",
    "pos = cnt(pos_words)\n",
    "\n",
    "# 負面詞彙頻率\n",
    "neg = cnt(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在負面評論關鍵詞中尋找正面關鍵詞\n",
    "for word, count in pos.most_common(250):\n",
    "    negc = neg[word]\n",
    "    if abs((count-negc)/count) > 0.50:\n",
    "        print(word, count, negc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在正面評論關鍵詞中尋找負面關鍵詞\n",
    "for word, count in neg.most_common(250):\n",
    "    posc = pos[word]\n",
    "    if abs((count-posc)/count) > 0.50:\n",
    "        print(word, count, posc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用單純貝氏分類器 (Naive Bayes Classifier) 搜尋關鍵詞\n",
    "本範例使用的單純貝氏分類器是來自於NLTK (Natural Language Toolkit)，其詳細原始碼與計算公式可參閱下列網址之套件官方文檔：\n",
    "\n",
    "[https://www.nltk.org/_modules/nltk/classify/naivebayes.html](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)\n",
    "\n",
    "- 重點計算方式摘錄\n",
    "```\n",
    "\"\"\"\n",
    "A classifier based on the Naive Bayes algorithm.  In order to find the\n",
    "probability for a label, this algorithm first uses the Bayes rule to\n",
    "express P(label|features) in terms of P(label) and P(features|label):\n",
    "\n",
    "|                       P(label) * P(features|label)\n",
    "|  P(label|features) = ------------------------------\n",
    "|                              P(features)\n",
    "\n",
    "The algorithm then makes the 'naive' assumption that all features are\n",
    "independent, given the label:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                                         P(features)\n",
    "\n",
    "Rather than computing P(features) explicitly, the algorithm just\n",
    "calculates the numerator for each label, and normalizes them so they\n",
    "sum to one:\n",
    "\n",
    "|                       P(label) * P(f1|label) * ... * P(fn|label)\n",
    "|  P(label|features) = --------------------------------------------\n",
    "|                        SUM[l]( P(l) * P(f1|l) * ... * P(fn|l) )\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 關鍵詞總和\n",
    "tot_words = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部關鍵詞中的前20個頻率較高的關鍵詞\n",
    "tot_words.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取前1000個頻率較高的關鍵詞\n",
    "top1k = [x for (x, y) in tot_words.most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將top1k關鍵詞特徵化之函式\n",
    "def featurize(review, topk=top1k, stopw=en_stopw):\n",
    "    tok_rev = wt(review)\n",
    "    rev_word = [word for word in tok_rev if word not in stopw]\n",
    "    features = {}\n",
    "    for word in top1k:\n",
    "        features['contains({})'.format(word)] = (word in rev_word)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將要輸入至單純貝氏分類器的資料轉換成向量\n",
    "train = [(featurize(rev), senti) for (rev, senti) in zip(preprocessed.review, preprocessed.sentiment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練/執行單純貝氏分類器\n",
    "nbclassifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示前100個出現機率較高的關鍵詞\n",
    "# 0: negative sentiment, 1: positive sentiment\n",
    "nbclassifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT\n",
    "\n",
    "`KeyBERT` 是一個用預訓練模型來尋找關鍵詞的方法。其特點是利用可利用BERT模型或是Transformer架構模型來尋找關鍵詞。\n",
    "而其尋找關鍵詞的排序方法則是以計算`Cosine Similarity (餘弦相似度)`來做排序。\n",
    "\n",
    "詳細可參閱官方網站暨官方文檔：\n",
    "[https://maartengr.github.io/KeyBERT/guides/quickstart.html](https://maartengr.github.io/KeyBERT/guides/quickstart.html)\n",
    "\n",
    "附圖為 `KeyBERT` 方法架構圖：\n",
    "\n",
    "![KeyBERT Architecture](https://i.imgur.com/2G3v6jT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇並載入預訓練模型\n",
    "# 其預訓練模型可從 HuggingFace 網站上取得, 或是Sentence-transformer (SBERT)官方文檔中所列的模型\n",
    "# 本範例使用的是 distilBERT-base-uncased 的模型\n",
    "sentence_model = SentenceTransformer(\"distilbert-base-uncased\", device=\"cuda\")\n",
    "\n",
    "memo_ = preprocessed\n",
    "kw_model = KeyBERT(sentence_model)  # Instantiate KeyBERT model\n",
    "n_keywords = 100 # Specify number of keywords to extract\n",
    "ngram = 1  # Specify ngram of keywords\n",
    "\n",
    "# Apply KeyBERT model extraction function along 'Text' axis of pandas dataframe\n",
    "memo_keywords_df = memo_['review'].apply(lambda x:\n",
    "                                       kw_model.extract_keywords(x,\n",
    "                                                                 keyphrase_ngram_range=(1, ngram),\n",
    "                                                                 stop_words='english',\n",
    "                                                                 highlight=False,\n",
    "                                                                 top_n=n_keywords))\n",
    "# Display results\n",
    "for i, memo_keywords in enumerate(memo_keywords_df):\n",
    "    print(\"-\"*40 + \"\\nmemo_ #{}: top {} keywords (ngram range 1-{})\".format(i, n_keywords, ngram))\n",
    "    for keyword in memo_keywords:\n",
    "        print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
